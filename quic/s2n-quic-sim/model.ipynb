{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba9ef20-ea51-442d-88ca-964020080775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, BoolTensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70d13e2-c756-4787-ae6d-bd8ac6c9036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer import TransformerBlock, CausalAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa03b433-1ca0-47d9-b9f7-b60d660aa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.time2vec import SineActivation, CosineActivation\n",
    "\n",
    "VALID_T2V_ACTIVATION = [\"sin\", \"cos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d6142d-1a76-4775-8954-8788ddb4bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([4, 5, 1])\n",
      "Slice shape: torch.Size([4, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose we have a tensor of shape (B, S, C)\n",
    "B, S, C = 4, 5, 1  # Example dimensions\n",
    "tensor = torch.randn(B, S, C)  # Create a random tensor\n",
    "\n",
    "# Extract the slice [:, :, 0] and retain the singleton dimension\n",
    "slice_tensor = tensor[:, :, 1:]\n",
    "\n",
    "# Check the shape of the resulting tensor\n",
    "print(\"Original shape:\", tensor.shape)  # (B, S, C)\n",
    "print(\"Slice shape:\", slice_tensor.shape)  # (B, S, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfc106ad-84ac-446a-a99b-4bf2fb7157cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCCT(nn.Module):\n",
    "    \"\"\"QUIC Congestion Control Transformer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        hidden_size: int,\n",
    "        n_heads: int,\n",
    "        n_layers: int,\n",
    "        expand_size: int,\n",
    "        context_size: int,\n",
    "        t2v_act: str = \"sin\",\n",
    "        act: nn.Module = nn.GELU,\n",
    "        attention: nn.Module = CausalAttention,\n",
    "        drop: float = 0.1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Features:\n",
    "        # 1.1 timestamp\n",
    "        if t2v_act == \"sin\":\n",
    "            self.t2v = SineActivation(1, hidden_size)\n",
    "        elif t2v_act == \"cos\":\n",
    "            self.t2v = CosineActivation(1, hidden_size)\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported activation:{t2v_act} for time2vec\")\n",
    "        # 1.2 other features\n",
    "        self.o2v = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(n_features - 1, expand_size, bias=bias),\n",
    "                act(),\n",
    "                nn.Linear(expand_size, hidden_size, bias=bias),\n",
    "                nn.Dropout(drop),\n",
    "            ]\n",
    "        )\n",
    "        # 1.3 feature dropout\n",
    "        self.f_drop = nn.Dropout(drop)\n",
    "\n",
    "        # 2. transformer blocks\n",
    "        # initialize num_layers of transformer layers\n",
    "        self.tfm_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_heads=n_heads,\n",
    "                    context_size=context_size,\n",
    "                    expand_size=expand_size,\n",
    "                    attention=attention,\n",
    "                    act=act,\n",
    "                    bias=bias,\n",
    "                    attn_drop=drop,\n",
    "                    out_drop=drop,\n",
    "                    ffn_drop=drop,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 3. output\n",
    "        self.final = nn.Linear(hidden_size, 1, bias=bias)\n",
    "\n",
    "        # 4. init parameters\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # [Input]: (B, S, C)\n",
    "        # B: batch_size, S: n_events, C: n_features\n",
    "        B, S, C = x.shape\n",
    "        # Step 1: (B, S, C) -> (B, S, D)\n",
    "        # B: batch_size, S: n_events, D: hidden_size\n",
    "\n",
    "        # Step 1.1: timestamp\n",
    "        # (B, S, 1)\n",
    "        timestamp = x[:, :, 0].unsqueeze(-1)\n",
    "        # (B, S, D)\n",
    "        f_ts = self.t2v(timestamp)\n",
    "\n",
    "        # Step 1.2: other features\n",
    "        # (B, S, C-1)\n",
    "        f_others = x[:, :, 1:]\n",
    "        # (B, S, D)\n",
    "        for layer in self.o2v:\n",
    "            f_others = layer(f_others)\n",
    "\n",
    "        # Step 1.3: Addition\n",
    "        f_all = self.f_drop(f_ts + f_others)\n",
    "\n",
    "        # Step 2: transformer blocks\n",
    "        for block in self.tfm_blocks:\n",
    "            f_all = block(f_all)\n",
    "\n",
    "        # Step 3: next congestion control window\n",
    "        return self.final(f_all)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if module._get_name() == \"fc2\":\n",
    "                # GPT-2 style FFN init\n",
    "                torch.nn.init.normal_(\n",
    "                    module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.num_layers)\n",
    "                )\n",
    "            else:\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f97f543c-31a2-4ea3-9223-19d60c312bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 8\n",
    "hidden_size = 64\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "expand_size = 128\n",
    "context_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbb38c9a-cabe-45bb-9afe-a4f910577901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QCCT(\n",
       "  (t2v): SineActivation()\n",
       "  (o2v): ModuleList(\n",
       "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (f_drop): Dropout(p=0.1, inplace=False)\n",
       "  (tfm_blocks): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalAttention(\n",
       "        (Wqkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (Wo): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FeedForward(\n",
       "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QCCT(\n",
    "    n_features=n_features,\n",
    "    hidden_size=hidden_size,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    expand_size=expand_size,\n",
    "    context_size=context_size,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "373c16a5-c5e1-4c45-b8ce-9b663a7ea907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6939, -0.1266,  0.1713,  1.2234,  0.2717,  0.8448, -1.5471,\n",
       "          -0.5695]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, S, C = 1, 1, 8  # Example dimensions\n",
    "tensor = torch.randn(B, S, C)  # Create a random tensor\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52d92b7e-9f5c-4c2b-8f4b-5e2678a58c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2523]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99b531a-a121-45d6-b531-507f61e601ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTForCausalLM(GPT):\n",
    "    def __init__(self, loss_fn: nn.Module = nn.CrossEntropyLoss(), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # the labels are the next token, so shift the labels over one\n",
    "        # & resize inputs to same length as labels by dropping last token\n",
    "        inputs = x[:, :-1]\n",
    "        labels = x[:, 1:]\n",
    "\n",
    "        # logits are of shape batch, sequence length, vocab size (B, S, VS),\n",
    "        # labels are of shape batch, vocab size (B, S)\n",
    "        logits = super().forward(inputs)\n",
    "\n",
    "        # flatten logits into (B*S, VS) and labels into (B*S) & calculate loss\n",
    "        loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "\n",
    "        # return both the logits and the loss\n",
    "        return {\"logits\": logits, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363431aa-3103-438d-a7d9-6afc7a8b9252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
